{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ffa4b72",
   "metadata": {},
   "source": [
    "# Using APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abe260c",
   "metadata": {},
   "source": [
    "## 1. Using the Twitter API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3f922d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889372c0",
   "metadata": {},
   "source": [
    "### 1.1: Pulling all tweets based on a search query with the v1.1 API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc7a828",
   "metadata": {},
   "source": [
    "With the Twitter API we can access most of Twitterâ€™s functionality from within Python (that means both reading **and** writing Tweets, or finding out about users and trends). The package of choice is *Tweepy*, which deals with some of the messy details.\n",
    "\n",
    "To access the Twitter API, you need to be authenticated. Hence, every request has to come with authentication information. To get this information in the first place, we need to generate our own credentials with a Developer Account:\n",
    "\n",
    "1. Go to the <a href=https://developer.twitter.com/en>Twitter Developer Site</a> and apply for a Developer Account (you will need a Twitter account for this). If you want to use the v1.1 APIs you will also need to apply for elevated access (the v2.0 API can be used with just essential access)\n",
    "2. Create an application (e.g., \"My_first_application\"). Credentials and limits are per application, not per account.\n",
    "3. Once you have created your application, you can transfer your consumer API key and secret, as well as your app access key and secret to the Python code below (see also https://developer.twitter.com/en/docs/basics/authentication/overview/oauth). This will be needed for the v1.1 APIs. For the v2.0 API it suffices to use the \"bear token\"\n",
    "\n",
    "You can directly add your data as a string like this:\n",
    "```\n",
    "CONSUMER_API_KEY = 'COPY STRING HERE'\n",
    "CONSUMER_API_SECRET = 'COPY STRING HERE'\n",
    "ACCESS_KEY = 'COPY STRING HERE'\n",
    "ACCESS_SECRET = 'COPY STRING HERE'\n",
    "```\n",
    "\n",
    "So that I can share my code without everyone using my credentials (which would probably lead to me being blocked by Twitter), I'm instead reading the data from a csv here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c2b6da2",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'API_access.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m api_access \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAPI_access.csv\u001b[39m\u001b[38;5;124m'\u001b[39m,delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m CONSUMER_API_KEY \u001b[38;5;241m=\u001b[39m api_access[api_access[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapi\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtwitter_consumer_api_key\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkey\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      3\u001b[0m CONSUMER_API_SECRET \u001b[38;5;241m=\u001b[39m api_access[api_access[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapi\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtwitter_consumer_api_secret\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkey\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/util/_decorators.py?line=208'>209</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/util/_decorators.py?line=209'>210</a>\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/util/_decorators.py?line=210'>211</a>\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/util/_decorators.py:317\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/util/_decorators.py?line=310'>311</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/util/_decorators.py?line=311'>312</a>\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/util/_decorators.py?line=312'>313</a>\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/util/_decorators.py?line=313'>314</a>\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/util/_decorators.py?line=314'>315</a>\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(inspect\u001b[39m.\u001b[39mcurrentframe()),\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/util/_decorators.py?line=315'>316</a>\u001b[0m     )\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/util/_decorators.py?line=316'>317</a>\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py?line=934'>935</a>\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py?line=935'>936</a>\u001b[0m     dialect,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py?line=936'>937</a>\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py?line=945'>946</a>\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py?line=946'>947</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py?line=947'>948</a>\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py?line=949'>950</a>\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py?line=601'>602</a>\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py?line=603'>604</a>\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py?line=604'>605</a>\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py?line=606'>607</a>\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py?line=607'>608</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py?line=1438'>1439</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py?line=1440'>1441</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py?line=1441'>1442</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1729\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py?line=1726'>1727</a>\u001b[0m     is_text \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py?line=1727'>1728</a>\u001b[0m     mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py?line=1728'>1729</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py?line=1729'>1730</a>\u001b[0m     f,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py?line=1730'>1731</a>\u001b[0m     mode,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py?line=1731'>1732</a>\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py?line=1732'>1733</a>\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py?line=1733'>1734</a>\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py?line=1734'>1735</a>\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py?line=1735'>1736</a>\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py?line=1736'>1737</a>\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py?line=1737'>1738</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py?line=1738'>1739</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py?line=1739'>1740</a>\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/common.py:857\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/common.py?line=851'>852</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/common.py?line=852'>853</a>\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/common.py?line=853'>854</a>\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/common.py?line=854'>855</a>\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/common.py?line=855'>856</a>\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/common.py?line=856'>857</a>\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/common.py?line=857'>858</a>\u001b[0m             handle,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/common.py?line=858'>859</a>\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/common.py?line=859'>860</a>\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/common.py?line=860'>861</a>\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/common.py?line=861'>862</a>\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/common.py?line=862'>863</a>\u001b[0m         )\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/common.py?line=863'>864</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/common.py?line=864'>865</a>\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/common.py?line=865'>866</a>\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'API_access.csv'"
     ]
    }
   ],
   "source": [
    "api_access = pd.read_csv('API_access.csv',delimiter=';')\n",
    "CONSUMER_API_KEY = api_access[api_access['api'] == 'twitter_consumer_api_key']['key'].tolist()[0]\n",
    "CONSUMER_API_SECRET = api_access[api_access['api'] == 'twitter_consumer_api_secret']['key'].tolist()[0]\n",
    "ACCESS_KEY = api_access[api_access['api'] == 'twitter_access_key']['key'].tolist()[0]\n",
    "ACCESS_SECRET = api_access[api_access['api'] == 'twitter_access_secret']['key'].tolist()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182b868c",
   "metadata": {},
   "source": [
    "We are also not allowed to request too many Tweets at the same time. There are per-day limits, as well as \"rate limits\" for 15-minute blocks. If you exceed your limits, you **will** get blocked for some time. For detailed information on the limits, check out https://developer.twitter.com/en/docs/rate-limits.\n",
    "In many cases, we can use the functionality of Tweepy to automatically delay calls in order to wait on the rate limit - but be aware that this doesn't always work, and we may need to manually add timeouts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fb87e1",
   "metadata": {},
   "source": [
    "We are now ready to create our verified interface (automatically waiting on our rate limit as necessary):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bb13a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(CONSUMER_API_KEY, CONSUMER_API_SECRET)\n",
    "auth.set_access_token(ACCESS_KEY, ACCESS_SECRET)\n",
    "api = tweepy.API(auth, wait_on_rate_limit = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1060fbf9",
   "metadata": {},
   "source": [
    "Let's download some tweets! \n",
    "We actually have different \"endpoints\" to choose from (each is essentially its own API). An overview of the v1.1 APis can be found here: https://docs.tweepy.org/en/stable/api.html\n",
    "\n",
    "We will use the standard search endpoint. Note that this endpoint only allows you to download tweets based on general queries from the past week. If you want to download older tweets, you will need to dowload the tweets of a particular account (see below), or use the 30-day endpoint, for example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec5644f",
   "metadata": {},
   "source": [
    "Let's search for tweets with the hash tag `\"#redbull\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812b57c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = api.search_tweets(q='#redbull',lang='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96705602",
   "metadata": {},
   "source": [
    "You can find details about the tweet objects at https://developer.twitter.com/en/docs/twitter-api/v1/data-dictionary/object-model/tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9f8a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in tweets:\n",
    "    print(\"Created at: \" + str(tweet.created_at))\n",
    "    print(\"User: \" + tweet.user.screen_name)\n",
    "    print(\"Followers: \" + str(tweet.user.followers_count))\n",
    "    print(\"Content: \" + tweet.text)\n",
    "    print(\"---------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e0124d",
   "metadata": {},
   "source": [
    "Are these all the tweets? What do you think?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c31096",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf2ff35",
   "metadata": {},
   "source": [
    "No! A simple search will only return a small number of tweets, similar to the first page of the search results on a website. Instead, we need to paginate all the results. For the v1.1 API, we can use the `Cursor` class of `tweepy`. The documentation is here: https://docs.tweepy.org/en/stable/v1_pagination.html. `Cursor` allows us to control how many items we want using `.items()`. If we want all that can be found through the relevant endpoint, we just leave out the number here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e7a32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in tweepy.Cursor(api.search_tweets,q=\"#redbull\",lang=\"en\").items(5):\n",
    "    print(\"Created at: \" + str(tweet.created_at))\n",
    "    print(\"User: \" + tweet.user.screen_name)\n",
    "    print(\"Followers: \" + str(tweet.user.followers_count))\n",
    "    print(\"Content: \" + tweet.text)\n",
    "    print(\"---------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6aa041c",
   "metadata": {},
   "source": [
    "When requesting tweets in this manner, the API will cut anything beyond 140 characters. That means, even if we search for tweets with #redbull, the tweet we receive may not contain the hashtag. However, we can add the parameter `tweet_mode='extended'` to our `tweepy.Cursor()` call. In this case, returned tweets no longer have a `.text` attribute, but a `.full_text` attribute\n",
    "\n",
    "(alternatively, we can \"hydrate\" tweets at any time, using just their ID (i.e. request the full text). You can thus use only the tweet ID to share your data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144eedfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in tweepy.Cursor(api.search_tweets,q=\"#redbull\",lang=\"en\", tweet_mode='extended').items(5):\n",
    "    print(\"Created at: \" + str(tweet.created_at))\n",
    "    print(\"User: \" + tweet.user.screen_name)\n",
    "    print(\"Followers: \" + str(tweet.user.followers_count))\n",
    "    print(\"Content: \" + tweet.full_text) # Note: when looking at extended tweets, there is no attribute `.text`\n",
    "    print(\"---------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d25f82e",
   "metadata": {},
   "source": [
    "### 1.2: Pulling tweets based on a search query with the v2.0 API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3a9a32",
   "metadata": {},
   "source": [
    "As mentioned above, we can access the v2.0 API without elevated access and by simply using a bearer token. `tweepy` also supports this API, making our life easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92e0d532",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'api_access' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m BEARER_TOKEN \u001b[38;5;241m=\u001b[39m api_access[api_access[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapi\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtwitter_bearer_token\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkey\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'api_access' is not defined"
     ]
    }
   ],
   "source": [
    "BEARER_TOKEN = api_access[api_access['api'] == 'twitter_bearer_token']['key'].tolist()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163e0621",
   "metadata": {},
   "source": [
    "With v2.0 we cannot use the `API` class anymore. Instead, we use the `Client` class, which is documented here: https://docs.tweepy.org/en/stable/client.html. The site also gives an overview of the different endpoints that you can access, similar to the documentation for the `API` class used in v1.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dbce84",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = tweepy.Client(bearer_token=BEARER_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb3f350",
   "metadata": {},
   "source": [
    "We will use the `search_recent_tweets` endpoint. Bear in mind that the `search_all_tweets` endpoint is only accessible with research access.\n",
    "\n",
    "When you search for tweets with the client, you need to specify a query, just as before. However, in v2.0, not all tweet information is delivered. Hence, we use `tweet_fields`; more information can be found here: https://docs.tweepy.org/en/stable/expansions_and_fields.html#tweet-fields-parameter.\n",
    "\n",
    "Moreover, we can only get up to 100 tweets per search (and can specify to obtain less using `max_results`). We will see below how to get more results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2cb568",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = client.search_recent_tweets(query=\"#redbull -is:retweet\",\n",
    "                                     tweet_fields=[\"created_at\",\"lang\"],\n",
    "                                     max_results=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0969b245",
   "metadata": {},
   "source": [
    "The key information is now within the `tweets.data` list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b8ed1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36716a0",
   "metadata": {},
   "source": [
    "Note that each element is an object and this object has the attributes that we specified in the `tweet_field` (it also has the `text`, which is not shortened in this version of the API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459befac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in tweets.data:\n",
    "    print(\"Created at: \" + str(tweet.created_at))\n",
    "    print(\"Language: \" + str(tweet.lang))\n",
    "    print(\"Content: \" + tweet.text)\n",
    "    print(\"---------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252b2497",
   "metadata": {},
   "source": [
    "However, a lot of data is missing. For example, the tweet doens't come with all the user information as it did in v1.1 (there will be an error in the next line):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca01b50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.data[0].user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a5bd69",
   "metadata": {},
   "source": [
    "Instead, tweets can return user-information as a \"child object\", but only if we request this. For this, we use the `expansions` parameter, requesting `'author_id'`. A list of expansions can be found here: https://developer.twitter.com/en/docs/twitter-api/expansions.\n",
    "\n",
    "Note that when we use the expansion `'author_id'`, we only get basic information about the user. We can extend what information we get about the user by specifying the `user_fields` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2f1515",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = client.search_recent_tweets(query=\"#redbull -is:retweet\",\n",
    "                                     tweet_fields=[\"created_at\",\"lang\"],\n",
    "                                     expansions=['author_id'],\n",
    "                                     user_fields=[\"description\",\"profile_image_url\"],\n",
    "                                     max_results=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8764d95",
   "metadata": {},
   "source": [
    "The user information is not stored within `tweets.data`, but instead within `tweets.includes` (which itself is a dictionary that can contain things related to any expansion - the key `'users'` will give us the user information as a list). Note that the whole object model is described here: https://developer.twitter.com/en/docs/twitter-api/data-dictionary/object-model/tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f3d099",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.includes['users']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cd2b86",
   "metadata": {},
   "source": [
    "The objects in our `'users'` list always contains some basic information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13738391",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.includes['users'][0].name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d51ec86",
   "metadata": {},
   "source": [
    "For each of these users, we can also get all the information that we requested through the `user_fields`. For example, the `profile_image_url`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f437ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.includes['users'][0].profile_image_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddd05bb",
   "metadata": {},
   "source": [
    "There is one important aspect to keep in mind: the API will only return as many users as have posted anything in the search. Hence, `tweets.includes['users']` might a shorter list than `tweets.data` (exactly if one or more users posted multiple tweets)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e90eb70",
   "metadata": {},
   "source": [
    "****\n",
    "So far, so good. The problem is that we only get 100 tweets. How can we get more? This is where the `Paginator` class comes in: https://docs.tweepy.org/en/stable/v2_pagination.html. This allows us to essentially do multiple requests and ensures that tweets are processed in order. Actually, this is very similar to the `Cursor` class from v1.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b98f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = tweepy.Paginator(client.search_recent_tweets,\n",
    "                              query=\"#redbull -is:retweet\",\n",
    "                              tweet_fields=[\"created_at\",\"lang\"],\n",
    "                              expansions=['author_id'],\n",
    "                              user_fields=[\"description\",\"profile_image_url\"],\n",
    "                              max_results=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5db813c",
   "metadata": {},
   "source": [
    "We get back a \"pages\", each containing 100 tweets contained within its data. Basically, each page is like one call to our `client` object. If we want to also get information on the client, we have to be a bit careful and match the `author_id` field of the tweet with the `id` field of the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0396d81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for page in pages:\n",
    "    page_users = {user.id: user for user in page.includes['users']} # We create a dictionary indexed by the user id to easily retrieve the full user object of each tweet\n",
    "    for tweet in page.data:\n",
    "        print(\"Created at: \" + str(tweet.created_at))\n",
    "        print(\"Language: \" + str(tweet.lang))\n",
    "        print(\"Content: \" + tweet.text)\n",
    "        print(\"User name: \" + page_users[tweet.author_id].name)\n",
    "        print(\"---------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7629491",
   "metadata": {},
   "source": [
    "### 1.3: Finding followers in v1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e122e951",
   "metadata": {},
   "source": [
    "We now want to learn more about the people (and company accounts) that follow Red Bull (as well as about whom they follow other than Red Bull). Let's start with finding some of Red Bull's followers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd70d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "followers_rb = []\n",
    "for follower in tweepy.Cursor(api.get_followers,screen_name=\"redbull\").items(5):\n",
    "    followers_rb.append(follower)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef84be9",
   "metadata": {},
   "source": [
    "A company like Red Bull has quite some followers and we would run into problems trying to get all at once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e213d0a7",
   "metadata": {},
   "source": [
    "Note that followers are saved as \"User\" objects, with their very own attributes, found here: https://developer.twitter.com/en/docs/twitter-api/v1/data-dictionary/object-model/user. The twitter-handle is defined by the `screen_name` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d0a5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "follower = followers_rb[0]\n",
    "follower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0657c33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "follower.screen_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6615a1e",
   "metadata": {},
   "source": [
    "Can we get other accounts that this person follows? (Twitter defines those as friends). Sometimes, the information is set to private, so we don't know who the person is following. Hence, we need to do some Exception management:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9489e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for user in tweepy.Cursor(api.get_friends, screen_name=follower.screen_name).items(10):\n",
    "        print(user.screen_name)\n",
    "except:\n",
    "    print(\"Follower \" + follower.screen_name + \" does not provide access to their friends.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844b0630",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Let's combine this for multiple of Red Bull's followers. For the first 5 followers, let's get up 10 of the accounts that they follow each. Can you add each of the (up to) 10 friends of 5 followers to a list?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b22221b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_followers_friends = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e572886",
   "metadata": {},
   "source": [
    "Once done, print out the screen name of the combined list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f3ef03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b5b9a8",
   "metadata": {},
   "source": [
    "It's easy to imagine how we could create a network of accounts, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85adf6de",
   "metadata": {},
   "source": [
    "### 1.4: Finding followers in v2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa294539",
   "metadata": {},
   "source": [
    "As with the search for tweets, we have to change things up a little bit in order to get information about followers and friends through v2.0. As before, we can use the `Client` class for up to 100 results, and the `Paginator` if we need more. We will stick to the `Client` class for this part, since we only need the followers for Red Bull. The endpoint of choice is `get_users_followers`. However, it requires a user ID as input, so we first have to find Red Bull's user ID. For this, we can make use of the `get_user` endpoint, which returns a user object, specified here: https://developer.twitter.com/en/docs/twitter-api/data-dictionary/object-model/user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb6df20",
   "metadata": {},
   "outputs": [],
   "source": [
    "rb = client.get_user(username=\"redbull\")\n",
    "rb_id = rb.data.id\n",
    "print(rb_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f9cd7d",
   "metadata": {},
   "source": [
    "We can now get the followers. Note that we will only get the base data with the following request (everything marked as \"default\" in the User object documentaiton). If we want more information, we can again use the `user_fields` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b4ba33",
   "metadata": {},
   "outputs": [],
   "source": [
    "followers_rb = client.get_users_followers(id=rb_id, max_results = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b1e28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for follower in followers_rb.data:\n",
    "    print(follower.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212f140b",
   "metadata": {},
   "source": [
    "The \"friends\" are now found through the `get_users_following` endpoint, which works in a very similar manner (it will also return a user object):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ec4a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "friends = client.get_users_following(id=followers_rb.data[0].id, max_results = 5)\n",
    "for friend in friends.data:\n",
    "    print(friend.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e92b7c",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Can you again combine the two pieces of code above to create a list containing 10 friends of each of the first 5 followers of Red Bull?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91929615",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_followers_friends = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb46082",
   "metadata": {},
   "source": [
    "Once done, print out the screen name of the combined list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5f6a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ca6197",
   "metadata": {},
   "source": [
    "### 1.5: Finding a specific user's tweets (over time) in v1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d931a88",
   "metadata": {},
   "source": [
    "We can also take a look at all the Tweets of a specific account. When looking at an account's Tweets, we do not have to worry about date limits (there are some limitations, however).\n",
    "\n",
    "To search an account's Tweets, we can use either the `.screen_name` or the `.user_id`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9726e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in tweepy.Cursor(api.user_timeline,user_id=rb_id).items(5):\n",
    "    print(tweet.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7296c0ea",
   "metadata": {},
   "source": [
    "### 1.6: Finding a specific user's tweets (over time) in v2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafb515a",
   "metadata": {},
   "source": [
    "The relevant endpoint in v2.0 is `get_users_tweets`, which is documented here: https://developer.twitter.com/en/docs/twitter-api/tweets/timelines/api-reference/get-users-id-tweets. This allows to find up to 3,200 tweets using the `Paginator` (or 100 using the `Client`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a70fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = client.get_users_tweets(id=rb_id,\n",
    "                                 tweet_fields=[\"created_at\",\"lang\"],\n",
    "                                 max_results=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c540810",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in tweets.data:\n",
    "    print(tweet.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1758a92a",
   "metadata": {},
   "source": [
    "### 1.7: Back to our engagement measures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6086d3",
   "metadata": {},
   "source": [
    "Let's try to enrich our `racingdf` using Tweet data.\n",
    "\n",
    "We can only collect tweets by hashtag for a week. Hence, I have prepared previous tweets for the last year (see on Moodle). This is stored as a `pickle` file - a system that allows to directly save arbitrary Python objects outside of our program. Once we call up the pickle file, we get back exactly the variables we saved into it. Since I saved a list of tweets, the return value from `pickle.load(file)` will be a list of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171ac547",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('red_bull_tweets.txt', 'rb') as file:\n",
    "    tweets = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2fe8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf32d33a",
   "metadata": {},
   "source": [
    "Note: this contains tweets (without retweets) for all the race days in the '\"red_bull_race_results.csv\"' file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b60ac4",
   "metadata": {},
   "source": [
    "Briefly recall / recreate our dataset `racingdf`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d32344",
   "metadata": {},
   "outputs": [],
   "source": [
    "racingdf = pd.read_csv('red_bull_race_results.csv')\n",
    "\n",
    "# Date formatting\n",
    "racingdf['date'] = pd.to_datetime(racingdf['date'], format=\"%d.%m.%y\")\n",
    "\n",
    "# \"out\"-indicator (adjusted on March 20)\n",
    "racingdf['perez_out'] = racingdf['perez'].isna().astype(int)\n",
    "racingdf['verstappen_out'] = racingdf['verstappen'].isna().astype(int)\n",
    "racingdf.loc[racingdf['date'] == '2022-03-20','perez_out'] = 1\n",
    "racingdf.loc[racingdf['date'] == '2022-03-20','verstappen_out'] = 1\n",
    "\n",
    "# Input placement for \"out\" days\n",
    "racingdf['perez'] = racingdf['perez'].fillna(racingdf['perez'].max())\n",
    "racingdf['verstappen'] = racingdf['verstappen'].fillna(racingdf['verstappen'].max())\n",
    "\n",
    "racingdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88353224",
   "metadata": {},
   "source": [
    "Let's take the first of these race dates and try to assign the corresponding tweets. Note that we need to convert the Twitter-specific time format into the same time format we've been using for our dates in the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0241617f",
   "metadata": {},
   "outputs": [],
   "source": [
    "racedate = racingdf['date'].iloc[0]\n",
    "print(\"Date: \" +  str(racedate))\n",
    "raceday_tweets = [tweet for tweet in tweets if pd.to_datetime(tweet.created_at.date()) == racedate]\n",
    "print(\"Total tweets: \" + str(len(raceday_tweets)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23952c4",
   "metadata": {},
   "source": [
    "Let's take a look at one of those tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba1082b",
   "metadata": {},
   "outputs": [],
   "source": [
    "raceday_tweets[11].text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8365831",
   "metadata": {},
   "source": [
    "The name \"Verstappen\" appears in here. We can, of course, check this automatically with Python (note that we use `.lower()` to avoid issues when comparing different capitalization):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df05ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "'verstappen' in raceday_tweets[11].text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69642c4e",
   "metadata": {},
   "source": [
    "We can combine the above code to create three new \"engagement\" columns: a total count of tweets on raceday, a count of tweets talking about Perez and a count of tweets talking about Verstappen, all relative to the count of tweets in the week around the race. We first create an empty column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c8bd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "racingdf['tweets_total'] = 0\n",
    "racingdf['tweets_perez'] = 0\n",
    "racingdf['tweets_verstappen'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8f5ebe",
   "metadata": {},
   "source": [
    "In order to create our relative metric, we again go through each racedate, and search for the relevant tweets in the week around there. To be more accurate when attributing tweets to Perez/Verstappen, we search both for last and first names.\n",
    "\n",
    "Keep in mind that this is only an initial proxy measure for engagement. And note that the code will run for a bit (runtime could be much improved - do you know how? We are not doing that here, to make clearer what the code is actually doing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50efcd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_dates = [pd.to_datetime(tweet.created_at.date()) for tweet in tweets] # We get a list of all tweet dates, so we don't have to recalculate them later\n",
    "for racedate in racingdf['date']:\n",
    "    perez_count_day = 0\n",
    "    perez_count_week = 0\n",
    "    verstappen_count_day = 0\n",
    "    verstappen_count_week = 0\n",
    "    total_count_day = 0\n",
    "    total_count_week = 0\n",
    "    for i in range(len(tweets)):\n",
    "        tweet_text = tweets[i].text.lower()\n",
    "        if tweet_dates[i] == racedate:\n",
    "            total_count_day += 1\n",
    "            if 'sergio' in tweet_text or 'perez' in tweet_text:\n",
    "                perez_count_day += 1\n",
    "            if 'max' in tweet_text or 'verstappen' in tweet_text:\n",
    "                verstappen_count_day += 1\n",
    "        if abs((tweet_dates[i] - racedate).days) <= 3:\n",
    "            total_count_week += 1\n",
    "            if 'sergio' in tweet_text or 'perez' in tweet_text:\n",
    "                perez_count_week += 1\n",
    "            if 'max' in tweet_text or 'verstappen' in tweet_text:\n",
    "                verstappen_count_week += 1\n",
    "    # The final measures are defined as tweet count on race day divided by average daily tweet count in the week around the race\n",
    "    racingdf.loc[racingdf['date'] == racedate,\"tweets_total\"] = total_count_day / (total_count_week / 7)\n",
    "    racingdf.loc[racingdf['date'] == racedate,\"tweets_perez\"] = perez_count_day / (perez_count_week / 7)\n",
    "    racingdf.loc[racingdf['date'] == racedate,\"tweets_verstappen\"] = verstappen_count_day / (verstappen_count_week / 7)\n",
    "racingdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d6d091",
   "metadata": {},
   "source": [
    "### Discussion point: Can you interpret these results? Why is the engagement around Verstappen so large on May 22, July 31, and October 9?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7062af3",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638086b1",
   "metadata": {},
   "source": [
    "We can, of course, do more analysis here. A good starting point is always to use visualization. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c69e78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x = 'verstappen',\n",
    "             y = 'tweets_verstappen',\n",
    "             data = racingdf,\n",
    "             hue=\"verstappen_out\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d666ec",
   "metadata": {},
   "source": [
    "We can get a more concrete picture by using regression analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910c503c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = racingdf[['verstappen','perez','verstappen_out','perez_out']]\n",
    "Y = racingdf.tweets_total\n",
    "X = sm.add_constant(X)\n",
    "lm = sm.OLS(Y,X).fit()\n",
    "print (lm.summary()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d098e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = racingdf.tweets_perez\n",
    "lm = sm.OLS(Y,X).fit()\n",
    "print (lm.summary()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4c8c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = racingdf.tweets_verstappen\n",
    "lm = sm.OLS(Y,X).fit()\n",
    "print (lm.summary()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10db0b00",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### 1.8 (Exercise): Finding out more about the people talking about Verstappen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04194b36",
   "metadata": {},
   "source": [
    "**Finding the right tweets and users**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbbde70",
   "metadata": {},
   "source": [
    "Start by finding all the tweets in which the word `'verstappen'` appears, making sure to eliminate any capitalizaiton issues. Put those tweets into a list `verstappen_tweets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1032ddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "verstappen_tweets = []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354c0b6b",
   "metadata": {},
   "source": [
    "Next, find out how many tweets each user made that made any of the `verstappen_tweets`.\n",
    "\n",
    "One possibile approach is to create a dictionary of tweet-lists, loop through the tweets, and change the dictionary as follows: if the `.user.screen_name` attribute has not appeared before, create a new entry into the dictionary. The attribute is the key and as a value, create a new list with the current tweet inside. If the `.user.screen_name` attribute has appeared before, simply append the current tweet to the corresponding list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095da5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_dict = {}\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e8d690",
   "metadata": {},
   "source": [
    "Next, create a list of `active_tweeters` and a list of `inactive_tweeters`. The former list should contain the users with more than one tweet within `verstappen_tweets`.\n",
    "\n",
    "Note: it will be useful later on if you store the `.user`-objects, not just the `.user.screen_name` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496c9087",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_tweeters = []\n",
    "inactive_tweeters = []\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d07d80d",
   "metadata": {},
   "source": [
    "How many `active_tweeters` are there? How many `inactive_tweeters`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6beb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256155bf",
   "metadata": {},
   "source": [
    "**Counting followers**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79a07d8",
   "metadata": {},
   "source": [
    "Next, we will take a look at the followers of our different tweeters. For the active (resp. inactive) tweeters, display a histogram showing the number of followers. The relevant user-attribute is `.followers_count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc82bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "followers_active = []\n",
    "followers_inactive = []\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606c302e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320eb167",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0672daf4",
   "metadata": {},
   "source": [
    "The extremely skewed nature of the number of followers makes it difficult to see anything or make comparisons. When we have heavily skewed data, we usually use the logarithm instead. Hence, repeat the plotting exercise with the `np.log()` of the `.followers_count`. Keep in mind that some may have 0 followers, so add a 1 to avoid errors. That is, use `np.log(original_value + 1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20ebed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "followers_active = []\n",
    "followers_inactive = []\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2f20f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ff2247",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc3f222",
   "metadata": {},
   "source": [
    "Do you see any differences?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc65743d",
   "metadata": {},
   "source": [
    "**Analyzing highly influential followers**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2eae4b8",
   "metadata": {},
   "source": [
    "Next, we will take a look at the `active_tweeters` with more than $10,000$ followers. Create a new list, `selected_accounts`, and store the relevant user-objects within the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb2cf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_accounts = [user for user in active_tweeters if user.followers_count > 10000]\n",
    "for user in selected_accounts:\n",
    "    print(\"User \" + user.screen_name + \" writes about Verstappen and has \" + str(user.followers_count) + \" followers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f42685",
   "metadata": {},
   "source": [
    "To understand the type of highly influential followers better, we take a look at the tweets of the `selected_accounts`. In particular, we explore the hashtags that they use.\n",
    "\n",
    "1. Create a list of hashtags\n",
    "2. loop through the `selected_accounts`\n",
    "3. For each user, find the last 5 (complete) tweets they wrote (using `tweepy.Cursor(api.user_timeline,user_id=user.id, tweet_mode='extended').items(5)` in v1.1 or `client.get_users_tweets(id=user.id,max_results=5) in v2.0)\n",
    "4. Within each tweet, collect the list of hashtags (using `.entities['hashtags']` - in the case of v2.0 we have to specifically the request the `entities` sub-object) and append these to our overall list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092d8d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hashtags = []\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab19d5e",
   "metadata": {},
   "source": [
    "If you print out the list of hashtags, you'll see that each hashtag is a dictionary with two keys:\n",
    "1. `'text'`: this gives the actual hashtag\n",
    "2. `'indices'`: this gives the position of the hashtag within the tweet\n",
    "\n",
    "Go through the list of hashtags and store only the actual hashtag using key `'text'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618d0440",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c4dbc6",
   "metadata": {},
   "source": [
    "Finally, add the hashtags into a dictionary, together with the number of times they appear (using the function `.groupby()` of your newly created data frame). Sort the dataframe by the occurence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17fc19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3038f75c",
   "metadata": {},
   "source": [
    "What types of influential accounts do you think actively post about Red Bull?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f2b059",
   "metadata": {},
   "source": [
    "Similar to hashtags, we can also find out which users are being mentioned in tweets (the @'s), using `.entities['user_mentions']`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c84cb9",
   "metadata": {},
   "source": [
    "### 1.9 (Exercise): A brief look at text analysis in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b83e3e",
   "metadata": {},
   "source": [
    "\n",
    "**Text analysis**\n",
    "\n",
    "We will next use some basic text analytics tools to find out more about what people have to say.\n",
    "We can start with the most used words. This gives a sense of how people are perceiving Red Bull. We can easily split tweets into words using `.split()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03088e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = tweets[0]\n",
    "tweet.text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f864be",
   "metadata": {},
   "source": [
    "Let's use this to generate a complete list of (lowercase) words (without hashtags). Remember, to get lowercase, you can apply `.lower()` to a string. You can get rid of hashtags using `.replace('#','')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc6e26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e610cfac",
   "metadata": {},
   "source": [
    "Can you find out which ones are the 15 most frequently occuring words? There are many ways to do this, but the one with least code is to create a dataframe, then apply `.groupby('group_column')['group_column'].count().reset_index(name='count')` to the datafframe, and finally sort it by the `'count'` column, using `.sort_values(by='count',ascending=False).head(number to display)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fc2f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c91600",
   "metadata": {},
   "source": [
    "There is a lot of junk here. One first attempt to clean up this table is to remove all English stopwords (the most common words like \"the\" and \"a\"). Many libraries can do this for us, such as `nltk`. But if we haven't used `nltk` before, we need to download the stopword library first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818e2dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02929bc2",
   "metadata": {},
   "source": [
    "The English stopwords shouldn't be all too surprising:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af08fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206e7361",
   "metadata": {},
   "source": [
    "We can now remove all the (English) stopwords from the data frame. For this, you want to make sure that you remove any row from your dataframe, where the word is not in the list of stopwords. That is, you can use `df[~(df['word_column'].isin(list of stopwords))]`. Afterwards, use the code from before to sort the table and display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c4332a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1858ae1c",
   "metadata": {},
   "source": [
    "*****\n",
    "\n",
    "**Interlude: Sentiment analysis introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f469bf47",
   "metadata": {},
   "source": [
    "Beyond counting words, there are fantastic tools out there to analyze sentinments. Usually, we need to start by training a sentiment analyzer. Luckily, `nltk` comes with an in-built pre-trained sentiment analyzer (VADER), purpose-built for analyzing short text on social media (convenient, right?). To use it for the first time, we first have to download its lexicon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ffc1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8189b70a",
   "metadata": {},
   "source": [
    "Let's see how it works by handing it a short sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d936b82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "sia.polarity_scores(\"My outlook on life is fantastic!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafc791e",
   "metadata": {},
   "source": [
    "The negative, neutral, and positive scores are self-explanatory and numbers here are between $0$ and $1$, with the total adding up to 1. The compound score follows a somewhat complex arithmetic, but it's easy to understand how to use it: it's between $-1$ and $1$, anything $>0$ signifies a positive sentiment, and anything $<0$ signifies a negative sentiment.\n",
    "\n",
    "Can it tell us something about clichÃ© optimists and pessimists?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba667c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sia.polarity_scores(\"The glass is half full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535a9585",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia.polarity_scores(\"The glass is half empty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8988be",
   "metadata": {},
   "source": [
    "What do you think? Does it make sense to rate the first sentence as neutral and the second one as negative?\n",
    "\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbb3427",
   "metadata": {},
   "source": [
    "Now let's apply this simple sentiment analysis to our tweets. Take any tweet and compute the \"compound\" score for that tweet (hint: when you apply `sia.polarity_scores` it returns a dictionary of scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14732732",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dc1399",
   "metadata": {},
   "source": [
    "Next, iteratre through all tweets, finding the compound score of each tweet and then displaying a histogram of compound scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1a38f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e649b23e",
   "metadata": {},
   "source": [
    "Of course, this is just a an initial look at sentiment analysis. You will see some more of this later in the module and in future modules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4401b1",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda52c9b",
   "metadata": {},
   "source": [
    " # 2. Connecting manually to an API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd39f0c9",
   "metadata": {},
   "source": [
    "We will see here how to connect to an API without the help of a wrapper package such as `tweepy`. We will use the example of Twitter, but this should give you an idea about requesting data from APIs more generally - it is essentially like requesting a website! Hence, we will use `requests`. If you want to learn more, I recommend this blogpost to get started: https://www.dataquest.io/blog/python-api-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b4549b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbb978f",
   "metadata": {},
   "source": [
    "On the Twitter Developer Platform under https://developer.twitter.com/en/docs/api-reference-index#Twitter, you can find the different API \"endpoints\" that Twitter provides (essentially, there is a different API depending on what data you want). We will be using here version 2.0 and searching for (recent) tweets. Hence, we follow the link https://developer.twitter.com/en/docs/twitter-api/tweets/search/api-reference/get-tweets-search-recent. Here, we find an \"Endpoint URL\", which provides the relevant data: 'https://api.twitter.com/2/tweets/search/recent' (essentially, the location of the API application on Twitter's server). We can send a request to this site, but we won't get much of a response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f880c91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.get(\"https://api.twitter.com/2/tweets/search/recent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464698ce",
   "metadata": {},
   "source": [
    "Why? because we haven't authenticated ourselves (response 401 indicates that we are not authorized). As we are using version 2.0, we can just use the Bearer Token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6a876a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEARER_TOKEN = api_access[api_access['api'] == 'twitter_bearer_token']['key'].tolist()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95ae9e8",
   "metadata": {},
   "source": [
    "We use the token to form a \"header\", which tells the server who we are. https://developer.twitter.com/apitools/api?endpoint=%2F2%2Ftweets%2Fsearch%2Frecent&method=get shows us how to build requests (it doesn't have code for Python as of now, but we can easily make sense of the curl code):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2852d194",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"Authorization\": \"Bearer {}\".format(BEARER_TOKEN),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e080a411",
   "metadata": {},
   "source": [
    "We also specify parameters: this is what we are looking for! This corresponds to the query parameter we used in the `tweepy.Client`. At https://developer.twitter.com/en/docs/twitter-api/tweets/search/api-reference/get-tweets-search-recent, you can find all the possible parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279a7628",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters={\n",
    "    'query': '#redbull -is:retweet',\n",
    "    'tweet.fields' : \"created_at,lang\",\n",
    "    'max_results' : 10\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efd0872",
   "metadata": {},
   "source": [
    "We can now retry our request with the header and the search parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8380ec72",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://api.twitter.com/2/tweets/search/recent\",\n",
    "                        headers = headers,\n",
    "                        params = parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109bc026",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3660495b",
   "metadata": {},
   "source": [
    "This time, we get a better outcome: 200 indicates that the request was accepted by the server and we get a \"normal\" response. Of course, we can now print this out. We use the fact that the API returns information in JSON format (this is the case for most modern APIs. Older APIs tend to return XMLs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acacef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_data = response.json()\n",
    "response_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9cd5fd",
   "metadata": {},
   "source": [
    "This looks familiar, right? In fact, we can look at the typical tweet attributes we already learned about (just that we now have dictionary notation, instead of attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa72b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_data['data'][0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37bc57b",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adb1f6b",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518a3d3e",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f597ba9a",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad129e6",
   "metadata": {},
   "source": [
    "## 3. The Reddit API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0c26ee",
   "metadata": {},
   "source": [
    "Similar to Twitter, Reddit provides an API that allows us to access a lot of data directly from Python. While we can use`praw` as a wrapper for our requests, we will access the API manually for training purposes.\n",
    "\n",
    "Note that Reddit is a bit more liberal with the allowable number of requests than Twitter. You can make up to 60 requests per minute (with a single request returning up to 100 posts). More information can be found <a href='https://github.com/reddit-archive/reddit/wiki/API'>here</a>.\n",
    "\n",
    "To access the Reddit API, we again need to authenticate ourselves:\n",
    "\n",
    "1. Go to the <a href=https://www.reddit.com/prefs/apps>Reddit Apps Site</a> and \"create another app\" (you will need a Reddit account for this).\n",
    "2. Create an application (e.g., \"dtvc_bot\"). The best option for our purposes is \"script\".\n",
    "3. Read the <a href=https://docs.google.com/a/reddit.com/forms/d/e/1FAIpQLSezNdDNK1-P8mspSbmtC2r86Ee9ZRbC66u929cG2GX0T9UMyw/viewform>terms and conditions</a> and register.\n",
    "4. With your application in hand, you can transfer your personal use script and secret token. However, you will also need your username and password to make this work.\n",
    "\n",
    "As before, I'm transferring all data from a csv-file, but feel free to input your data directly as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec57705",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_access = pd.read_csv('API_access.csv',delimiter=';')\n",
    "PERSONAL_USE_SCRIPT = api_access[api_access['api'] == 'reddit_personal_use_script']['key'].tolist()[0]\n",
    "SECRET_TOKEN = api_access[api_access['api'] == 'reddit_secret_token']['key'].tolist()[0]\n",
    "USERNAME = api_access[api_access['api'] == 'reddit_username']['key'].tolist()[0]\n",
    "PASSWORD = api_access[api_access['api'] == 'reddit_password']['key'].tolist()[0]\n",
    "USER_AGENT = api_access[api_access['api'] == 'reddit_user_agent']['key'].tolist()[0] # This should be descriptive, such as 'testscript by u/<Username>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac8e71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'grant_type': 'password', 'username': USERNAME, 'password': PASSWORD}\n",
    "headers = {'User-Agent': USER_AGENT}\n",
    "auth = requests.auth.HTTPBasicAuth(PERSONAL_USE_SCRIPT, SECRET_TOKEN)\n",
    "r = requests.post('https://www.reddit.com/api/v1/access_token',\n",
    "                        data=data,\n",
    "                        headers=headers,\n",
    "                        auth=auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c521552",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = r.json()\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e245af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = 'bearer ' + response['access_token']\n",
    "headers = {'Authorization': token, 'User-Agent': USER_AGENT}\n",
    "params = {'q': 'redbull', 'limit': 5, 'sort': 'relevance'}\n",
    "r = requests.get('https://oauth.reddit.com/subreddits/search', headers=headers, params=params)\n",
    "print(r.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa26c706",
   "metadata": {},
   "source": [
    "Unfortunately, the structure of the response is a bit messier than what we saw with Twitter. You will have to do some exploration of the documentation: https://www.reddit.com/dev/api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9196bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for subreddit in r.json()['data']['children']:\n",
    "    print(subreddit['data']['display_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77894ab3",
   "metadata": {},
   "source": [
    "Let's now try to find posts in all subreddits. To do so, we need to understand the reddit url structure: \"/r\" means we are searching in a specific subreddit, but \"/all\" is actually a placeholder to mean we are considering all possible subreddits. \"/new\" indicates the sorting. There are different ways to sort:\n",
    "* hot\n",
    "* controversial\n",
    "* gilded\n",
    "* new\n",
    "* rising\n",
    "* top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19aa88e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'q': 'redbull', 'limit': 5, 'sort': 'relevance'}\n",
    "r = requests.get('https://oauth.reddit.com/r/all/new', headers=headers, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecec509",
   "metadata": {},
   "outputs": [],
   "source": [
    "for submission in r.json()['data']['children']:\n",
    "    print('---------------')\n",
    "    print('Subreddit: ' + submission['data']['subreddit'])\n",
    "    print('Title: ' + submission['data']['title'])\n",
    "    print('Name: ' + submission['data']['author_fullname'])\n",
    "    print('upvote_ratio: ' + str(submission['data']['upvote_ratio']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b00160a",
   "metadata": {},
   "source": [
    "Since we can only get up to 100 items at a time, we have to be a bit more creative when requesting more. For this purpose, most of the endpoints have a `before` and `after` result, which allows you to link searches (and create your own paginator). Things are a bit easier if you use `praw`, however."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "14d2ea621ea2b11c0b4969644b7cab659547dfbf65fc061fadd77a658c79ffdf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
