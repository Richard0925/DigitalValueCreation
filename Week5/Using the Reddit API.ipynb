{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ad129e6",
   "metadata": {},
   "source": [
    "# Pulling posts through the Reddit API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169ffa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import praw\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0c26ee",
   "metadata": {},
   "source": [
    "Similar to Twitter, Reddit provides an API that allows us to access a lot of data directly from Python. This time, we use the library `praw` as a wrapper for our requests.\n",
    "\n",
    "Note that Reddit is a bit more liberal with the allowable number of requests than Twitter. You can make up to 60 requests per minute (with a single request returning up to 100 posts). More information can be found <a href='https://github.com/reddit-archive/reddit/wiki/API'>here</a>.\n",
    "\n",
    "To access the Reddit API, we again need to authenticate ourselves:\n",
    "\n",
    "1. Go to the <a href=https://www.reddit.com/prefs/apps>Reddit Apps Site</a> and \"create another app\" (you will need a Reddit account for this).\n",
    "2. Create an application (e.g., \"dtvc_bot\"). The best option for our purposes is \"script\".\n",
    "3. Read the <a href=https://docs.google.com/a/reddit.com/forms/d/e/1FAIpQLSezNdDNK1-P8mspSbmtC2r86Ee9ZRbC66u929cG2GX0T9UMyw/viewform>terms and conditions</a> and register.\n",
    "4. With your application in hand, you can transfer your personal use script and secret token. However, you will also need your username and password to make this work.\n",
    "\n",
    "As before, I'm transferring all data from a csv-file, but feel free to input your data directly as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec57705",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_access = pd.read_csv('API_access.csv',delimiter=';')\n",
    "PERSONAL_USE_SCRIPT = api_access[api_access['api'] == 'reddit_personal_use_script']['key'].tolist()[0]\n",
    "SECRET_TOKEN = api_access[api_access['api'] == 'reddit_secret_token']['key'].tolist()[0]\n",
    "USERNAME = api_access[api_access['api'] == 'reddit_username']['key'].tolist()[0]\n",
    "PASSWORD = api_access[api_access['api'] == 'reddit_password']['key'].tolist()[0]\n",
    "USER_AGENT = api_access[api_access['api'] == 'reddit_user_agent']['key'].tolist()[0] # This should be descriptive, such as 'testscript by u/<Username>'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bc76a3",
   "metadata": {},
   "source": [
    "We now create a link to Reddit using `praw`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c939f543",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(\n",
    "            client_id = PERSONAL_USE_SCRIPT,\n",
    "            client_secret = SECRET_TOKEN,\n",
    "            user_agent = USER_AGENT,\n",
    "            username = USERNAME,\n",
    "            password = PASSWORD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8fbae4",
   "metadata": {},
   "source": [
    "## Pulling Reddit data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d871d5",
   "metadata": {},
   "source": [
    "We start by pulling \"hot\" posts within any subreddit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a78f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for post in reddit.subreddit(\"all\").hot(limit=10):\n",
    "    print(post.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f7f930",
   "metadata": {},
   "source": [
    "We can also sort posts in other ways:\n",
    "* hot\n",
    "* controversial\n",
    "* gilded\n",
    "* new\n",
    "* rising\n",
    "* top"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f329638",
   "metadata": {},
   "source": [
    "We can only pull 100 posts at a time. Let's do that, get some of the key data, and put it into a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d36e790",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_df = pd.DataFrame()\n",
    "for post in reddit.subreddit(\"all\").new(limit=100):\n",
    "    post_df = post_df.append({\n",
    "        'subreddit': post.subreddit,\n",
    "        'title': post.title,\n",
    "        'upvote_ratio': post.upvote_ratio,\n",
    "        'score': post.score,\n",
    "        'created_utc': post.created_utc,\n",
    "        'fullname': post.fullname\n",
    "    }, ignore_index=True)\n",
    "post_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa91ea7",
   "metadata": {},
   "source": [
    "To get more data, we need to make multiple requests. It's a little bit tricky to ensure that we are not getting overlapping posts - we essentially have to ensure that we look at posts \"up to\" what we have found so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58637590",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "for i in range(3):\n",
    "    params['after'] = post_df.iloc[len(post_df)-1]['fullname']\n",
    "    posts = reddit.subreddit(\"all\").new(limit=100,params=params)\n",
    "    add_post_df = pd.DataFrame()\n",
    "    count = 0\n",
    "    for post in posts:\n",
    "        count += 1\n",
    "        add_post_df = add_post_df.append({\n",
    "                'subreddit': post.subreddit,\n",
    "                'title': post.title,\n",
    "                'upvote_ratio': post.upvote_ratio,\n",
    "                'score': post.score,\n",
    "                'created_utc': post.created_utc,\n",
    "                'fullname': post.fullname\n",
    "            }, ignore_index=True)\n",
    "    post_df = post_df.append(add_post_df, ignore_index=True)\n",
    "    print(\"Added \" + str(count) + \" posts\")\n",
    "print(post_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2875522e",
   "metadata": {},
   "source": [
    "If we want to search only hot posts, we have to work with time stamps, so things get a little bit more complicated. For example code, see <a href='https://github.com/praw-dev/praw/blob/50610754ffce4c2ce2ba416aba51f0f38744b2d4/praw/models/reddit/subreddit.py#L161'>here</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00d2b27",
   "metadata": {},
   "source": [
    "## Back to our Marketing campaign"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785ec656",
   "metadata": {},
   "source": [
    "Let's start by pulling a bit of data about Red Bull from the corresponding subreddit. In the setup below, we request the most recent 100 posts, then the 100 before that, and so on. If you want to crank up the search, remember the 60 requests/minute restriction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769f9c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_df = pd.DataFrame()\n",
    "params = {}\n",
    "for i in range(5):\n",
    "    posts = reddit.subreddit(\"redbull\").new(limit=100,params=params)\n",
    "    add_post_df = pd.DataFrame()\n",
    "    count = 0\n",
    "    for post in posts:\n",
    "        count += 1\n",
    "        add_post_df = add_post_df.append({\n",
    "                'subreddit': post.subreddit,\n",
    "                'title': post.title,\n",
    "                'text': post.selftext,\n",
    "                'upvote_ratio': post.upvote_ratio,\n",
    "                'score': post.score,\n",
    "                'img_link': post.url,\n",
    "                'created_utc': post.created_utc,\n",
    "                'fullname': post.fullname\n",
    "            }, ignore_index=True)\n",
    "    post_df = post_df.append(add_post_df, ignore_index=True)\n",
    "    print(\"Added \" + str(count) + \" posts\")\n",
    "    params['after'] = post_df.iloc[len(post_df)-1]['fullname']\n",
    "post_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863db847",
   "metadata": {},
   "source": [
    "Let's see when users have been most active. Note that time is given in Unix epoch. We can easily convert that to \"normal\" date and time using `pandas`;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f62967",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_df['created_utc'] = pd.to_datetime(post_df['created_utc'], unit='s')\n",
    "sns.histplot(post_df['created_utc'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc31acea",
   "metadata": {},
   "source": [
    "What about posts in the last week. Can we triangulate what we saw on Twitter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a40571",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_df[post_df['created_utc'] > (datetime.utcnow() - timedelta(days=7))].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f7a47a",
   "metadata": {},
   "source": [
    "Can you give two reasons why we don't see as much activity as on Twitter?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406ffb56",
   "metadata": {},
   "source": [
    "Let's try something new: in all subreddits, let's look at posts with Red Bull in the title (this may take a while). To make it comparable to our Twitter example, we will search only results of the last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1f4134",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_week_df = pd.DataFrame()\n",
    "params = {}\n",
    "for i in range(3):\n",
    "    posts = reddit.subreddit(\"all\").search(\"red bull\",sort=\"new\",time_filter=\"week\",params=params)\n",
    "    add_post_df = pd.DataFrame()\n",
    "    count = 0\n",
    "    for post in posts:\n",
    "        count += 1\n",
    "        add_post_df = add_post_df.append({\n",
    "                'subreddit': post.subreddit.title,\n",
    "                'title': post.title,\n",
    "                'text': post.selftext,\n",
    "                'upvote_ratio': post.upvote_ratio,\n",
    "                'score': post.score,\n",
    "                'img_link': post.url,\n",
    "                'created_utc': post.created_utc,\n",
    "                'fullname': post.fullname\n",
    "            }, ignore_index=True)\n",
    "    post_week_df = post_week_df.append(add_post_df, ignore_index=True)\n",
    "    print(\"Added \" + str(count) + \" posts\")\n",
    "    params['after'] = post_week_df.iloc[len(post_week_df)-1]['fullname']\n",
    "post_week_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c837d7",
   "metadata": {},
   "source": [
    "We will likely have some of the same posts in different subreddits. Let's filter those out, using the `groupby` function of `pandas`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e30cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_week_grouped_df = post_week_df.groupby('title').agg({'subreddit':'unique'}).reset_index()\n",
    "post_week_grouped_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcc3fba",
   "metadata": {},
   "source": [
    "As you know, many reddit posts contain relatively little text. But there are a lot of comments on pretty much everything. Of course, we don't have to stop at the posts, but we can also make use of the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc1add4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for post in reddit.subreddit(\"redbull\").new(limit=10):\n",
    "    post.comments.replace_more(limit=None)\n",
    "    for comment in post.comments:\n",
    "        print(comment.body)\n",
    "        print('-----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289cf471",
   "metadata": {},
   "source": [
    "When scrolling through Reddit, we often come across a \"More Comments\" button. Instead of getting back a `MoreComments` object, we use `replace_more()` to get rid of those. Note, however, that this allows only to replace up to 32 instances of \"More Comments\" appearing.\n",
    "\n",
    "Another issue is that we may not want to only look at the top-level comments, but we may want to see the replies to those comments, and the replies to those replies, etc.... You can think of the comment structure as a tree, so we will be searching everywhere through the tree (in what is called a \"breadth-first search\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4707ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for post in reddit.subreddit(\"redbull\").new(limit=10):\n",
    "    post.comments.replace_more(limit=None)\n",
    "    comment_queue = post.comments[:]\n",
    "    while len(comment_queue) > 0:\n",
    "        comment = comment_queue.pop(0)\n",
    "        print(comment.body)\n",
    "        comment_queue.extend(comment.replies)\n",
    "    print('-----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4bc3f3",
   "metadata": {},
   "source": [
    "Of course, we can do a lot of extra text analysis on this, also incorporating the structure of the comment tree."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dtvc_env",
   "language": "python",
   "name": "dtvc_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               